# ğŸ“ Projet : Recherche SÃ©mantique dâ€™Avis Ã‰tudiants

### (BM25 + BERT dans le cadre du cours de NLP)

---

## ğŸ§¾ Contexte pÃ©dagogique

Ce projet a Ã©tÃ© rÃ©alisÃ© dans le cadre du **cours de Natural Language Processing (NLP)**.  
L'objectif pÃ©dagogique Ã©tait de concevoir un systÃ¨me de recherche d'information performant combinant :

- ğŸ” **BM25**, une mÃ©thode de recherche lexicale (via Elasticsearch)
- ğŸ§  **Embeddings contextualisÃ©s**, Ã  lâ€™aide dâ€™un modÃ¨le de type BERT

> ğŸ‘‰ Le sujet nous invitait Ã  proposer une **solution hybride** capable de mieux interprÃ©ter le sens des phrases, en particulier dans le cadre dâ€™une **application concrÃ¨te**.

---

## ğŸ¯ Sujet du projet

Nous avons choisi de travailler sur le **traitement des avis dâ€™Ã©tudiants sur leurs cours**.  
Ces avis sont souvent :

- RÃ©digÃ©s en langage naturel (et donc subjectifs)
- Riches en ressentis, mais difficiles Ã  analyser automatiquement
- TrÃ¨s variÃ©s en formulation

### Notre mission :

> Concevoir un moteur de recherche capable de **retrouver automatiquement les avis les plus similaires** Ã  une phrase saisie, en **comprenant le sens**, pas uniquement les mots.

---

## ğŸ§  Les outils que nous avons utilisÃ©s

### ğŸ”¤ BERT : un modÃ¨le de langage contextuel

BERT (Bidirectional Encoder Representations from Transformers) est un modÃ¨le de langage dÃ©veloppÃ© par Google.  
Il comprend les phrases en tenant compte du **contexte des mots** (avant et aprÃ¨s).

ğŸ” Exemple :

> â€œCe cours manque de pratiqueâ€ â‰  â€œCâ€™est un cours trÃ¨s pratiqueâ€

---

### ğŸ§  Sentence Transformers : lâ€™encodage de phrases

Nous avons utilisÃ© la librairie `sentence-transformers` et le modÃ¨le :  
ğŸ“Œ `distiluse-base-multilingual-cased`

Ce modÃ¨le transforme une phrase en **vecteur numÃ©rique** (embedding), ce qui permet ensuite de :

- Comparer des phrases entre elles
- Mesurer leur **similaritÃ© sÃ©mantique** via la **cosine similarity**

---

### ğŸ” BM25 : la recherche classique dans les documents

BM25 est un algorithme utilisÃ© par les moteurs de recherche (notamment via **Elasticsearch**).

Il fonctionne en :

- Comparant les mots de la requÃªte Ã  ceux des documents
- Calculant une **pertinence lexicale** en fonction de la frÃ©quence des mots

> ğŸ“Œ Câ€™est rapide et efficaceâ€¦ mais Ã§a ne comprend pas le sens !

---

## ğŸ”€ La solution hybride que nous avons construite

Notre systÃ¨me combine :

1. âœ… **BM25**, pour une premiÃ¨re sÃ©lection rapide dâ€™avis pertinents
2. âœ… **Embeddings BERT**, pour reclasser les rÃ©sultats selon leur **proximitÃ© sÃ©mantique rÃ©elle**

### Ã‰tapes :

- Encodage de tous les avis existants avec le modÃ¨le `distiluse-base-multilingual-cased`
- Lors dâ€™une recherche :
  - La requÃªte est encodÃ©e
  - On compare son embedding Ã  ceux des avis existants
  - On classe les rÃ©sultats selon leur **score de similaritÃ© cosinus**

---

## ğŸ’¬ Cas dâ€™usage

| RequÃªte Ã©tudiante                       | RÃ©sultats retournÃ©s                     |
| --------------------------------------- | --------------------------------------- |
| â€œLe cours Ã©tait trop thÃ©oriqueâ€         | â€œIl manque de pratiqueâ€                 |
| â€œJe nâ€™ai rien compris aux explicationsâ€ | â€œLes explications Ã©taient trop rapidesâ€ |
| â€œBon professeurâ€                        | â€œTrÃ¨s pÃ©dagogue et clairâ€               |

âœ… Les rÃ©sultats sont pertinents **mÃªme quand les mots sont diffÃ©rents**, car le sens est compris.

---

## ğŸ“Š RÃ©sultats obtenus

Pour chaque requÃªte, le systÃ¨me affiche :

- ğŸ“„ Le texte de lâ€™avis retrouvÃ©
- ğŸ“˜ Le module concernÃ©
- ğŸ‘¤ Le nom de l'Ã©tudiant
- ğŸ“ˆ Un score de similaritÃ© (entre 0 et 1)

> Plus le score est proche de 1, plus lâ€™avis est pertinent par rapport Ã  la requÃªte.

---

## ğŸ§ª Exemple de requÃªtes testÃ©es

Le prof nâ€™explique pas bien  
Le cours est trop thÃ©orique  
Pas assez dâ€™exemples  
Trop de pratique  
Manque de structure  
Contenu bien expliquÃ©

---

## ğŸ”® Perspectives dâ€™Ã©volution

- CrÃ©ation dâ€™une **interface Web** pour rendre lâ€™outil accessible Ã  un plus large public
- GÃ©nÃ©ration automatique de **rapports pÃ©dagogiques** Ã  partir des avis
- IntÃ©gration de **lâ€™analyse de sentiment** (positif, nÃ©gatif, neutre)
- Extension Ã  dâ€™autres domaines : retour client, feedback produit, etc.

---

## ğŸ‘¥ Membres du groupe

Ce projet a Ã©tÃ© conÃ§u et rÃ©alisÃ© par notre groupe dans le cadre du cours de NLP :

- Moussa MallÃ©
- Arthur
- Maxies
- Adji Fatou
- Maurice

---

## ğŸ”— Lien vers le dÃ©pÃ´t GitHub

â¡ï¸ [https://github.com/codeangel223/npl---BERT-BM25](https://github.com/codeangel223/npl---BERT-BM25)

---

> Merci pour votre attention ğŸ™  
> Ce projet illustre lâ€™intÃ©rÃªt des approches hybrides en NLP pour mieux comprendre le **langage humain** dans des contextes rÃ©els.
